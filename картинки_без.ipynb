{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMUoLcHrlKhtXCRrT3B/zct",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2025airda-max/---/blob/main/%D0%BA%D0%B0%D1%80%D1%82%D0%B8%D0%BD%D0%BA%D0%B8_%D0%B1%D0%B5%D0%B7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "ac52VIcPzYKN",
        "outputId": "084e0c6e-7968-4f8c-b224-e203682a4c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2954646294.py:13: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), css=custom_css, title=\"SeTka Fooocus Launcher\") as demo:\n",
            "/tmp/ipython-input-2954646294.py:13: DeprecationWarning: The 'css' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'css' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft(), css=custom_css, title=\"SeTka Fooocus Launcher\") as demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7a37fa9c54be8a4fed.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7a37fa9c54be8a4fed.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!pip install -q gradio\n",
        "\n",
        "import gradio as gr\n",
        "\n",
        "# --- –î–ò–ó–ê–ô–ù (–°–ï–¢–ö–ê PROJECT) ---\n",
        "custom_css = \"\"\"\n",
        "#tg_btn { background-color: #0088cc !important; color: white !important; border: none !important; }\n",
        "#yt_btn { background-color: #ff0000 !important; color: white !important; border: none !important; }\n",
        ".gradio-container { font-family: 'Roboto', sans-serif; text-align: center; }\n",
        "\"\"\"\n",
        "\n",
        "# --- –ò–ù–¢–ï–†–§–ï–ô–° ---\n",
        "with gr.Blocks(theme=gr.themes.Soft(), css=custom_css, title=\"SeTka Fooocus Launcher\") as demo:\n",
        "        # –õ–û–ì–û–¢–ò–ü –ò –ó–ê–ì–û–õ–û–í–û–ö\n",
        "    with gr.Row():\n",
        "        gr.HTML(\"\"\"\n",
        "            <div style=\"text-align: center;\">\n",
        "                <h1 style=\"color: #ff2a2a; font-size: 48px; font-weight: 900; margin-bottom: 10px;\">SeTka Project</h1>\n",
        "                <h3 style=\"color: gold; text-shadow: 1px 1px 1px rgba(0,0,0,0.3);\">üöÄ SDXL PRO Launcher (Fooocus)</h3>\n",
        "                <img src=\"https://i.postimg.cc/Gt06QtT2/Se-Tka-Foto.jpg\" style=\"width: 150px; border-radius: 20px; border: 3px solid #ff2a2a; margin: 0 auto; display: block;\">\n",
        "            </div>\n",
        "        \"\"\")\n",
        "\n",
        "    # –ö–ù–û–ü–ö–ò –°–û–¶–°–ï–¢–ï–ô\n",
        "    with gr.Row():\n",
        "        btn_tg = gr.Button(value=\"‚úàÔ∏è Telegram –∫–∞–Ω–∞–ª\", link=\"https://t.me/SeTka_Project\", elem_id=\"tg_btn\")\n",
        "        btn_yt = gr.Button(value=\"‚ñ∂Ô∏è YouTube –∫–∞–Ω–∞–ª\", link=\"https://www.youtube.com/@SeTkaProjectMusic\", elem_id=\"yt_btn\")\n",
        "\n",
        "    gr.HTML(\"<hr>\")\n",
        "\n",
        "    # –ò–ù–§–û–†–ú–ê–¶–ò–Ø\n",
        "    gr.Markdown(\"### ‚úÖ –¢–æ–∫–µ–Ω –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è\")\n",
        "    gr.Markdown(\"–ü—Ä–æ—Å—Ç–æ –∑–∞–ø—É—Å—Ç–∏—Ç–µ **—Å–ª–µ–¥—É—é—â—É—é —è—á–µ–π–∫—É** (–Ω–∏–∂–µ), —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —É—Å—Ç–∞–Ω–æ–≤–∫—É –∏ –∑–∞–ø—É—Å–∫ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏.\")\n",
        "\n",
        "demo.launch(inline=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. –°–∫–∞—á–∏–≤–∞–Ω–∏–µ Fooocus (–µ—Å–ª–∏ –µ—â–µ –Ω–µ—Ç)\n",
        "if not os.path.exists(\"Fooocus\"):\n",
        "    print(\"‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞—é Fooocus (–ª—É—á—à–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è SDXL)...\")\n",
        "    !git clone https://github.com/lllyasviel/Fooocus.git\n",
        "\n",
        "%cd Fooocus\n",
        "\n",
        "print(\"üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏...\")\n",
        "# –°—Ç–∞–≤–∏–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Ç–∏—Ö–æ\n",
        "!pip install -r requirements_versions.txt > /dev/null\n",
        "\n",
        "print(\"üöÄ –ó–∞–ø—É—Å–∫–∞—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å... (–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –∑–∞–π–º–µ—Ç 5-7 –º–∏–Ω—É—Ç –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π)\")\n",
        "print(\"‚è≥ –ö–æ–≥–¥–∞ –ø–æ—è–≤–∏—Ç—Å—è —Å—Å—ã–ª–∫–∞ –≤–∏–¥–∞ 'Running on public URL: https://...gradio.live', –Ω–∞–∂–º–∏—Ç–µ –Ω–∞ –Ω–µ—ë.\")\n",
        "\n",
        "# –ó–∞–ø—É—Å–∫–∞–µ–º —Å –∞—Ä–≥—É–º–µ–Ω—Ç–æ–º --share (—ç—Ç–æ –∑–∞–º–µ–Ω—è–µ—Ç ngrok)\n",
        "!python entry_with_update.py --preset realistic --always-high-vram --share\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzZDs0MA0D2E",
        "outputId": "ab7dea9d-d8a7-49e2-8776-282cfba0ccb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚¨áÔ∏è –°–∫–∞—á–∏–≤–∞—é Fooocus (–ª—É—á—à–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ –¥–ª—è SDXL)...\n",
            "Cloning into 'Fooocus'...\n",
            "remote: Enumerating objects: 6735, done.\u001b[K\n",
            "remote: Total 6735 (delta 0), reused 0 (delta 0), pack-reused 6735 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6735/6735), 33.35 MiB | 15.16 MiB/s, done.\n",
            "Resolving deltas: 100% (3850/3850), done.\n",
            "/content/Fooocus\n",
            "üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏...\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-cloud-bigquery 3.40.0 requires packaging>=24.2.0, but you have packaging 24.1 which is incompatible.\n",
            "dataproc-spark-connect 1.0.1 requires tqdm>=4.67, but you have tqdm 4.66.4 which is incompatible.\n",
            "dataproc-spark-connect 1.0.1 requires websockets>=14.0, but you have websockets 11.0.3 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 11.0.3 which is incompatible.\n",
            "pytensor 2.36.3 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "db-dtypes 1.5.0 requires packaging>=24.2.0, but you have packaging 24.1 which is incompatible.\n",
            "access 1.1.10.post3 requires scipy>=1.14.1, but you have scipy 1.14.0 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "google-adk 1.21.0 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\n",
            "google-adk 1.21.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 11.0.3 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.0 which is incompatible.\n",
            "rasterio 1.5.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "mcp 1.25.0 requires httpx>=0.27.1, but you have httpx 0.27.0 which is incompatible.\n",
            "google-genai 1.55.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.0 which is incompatible.\n",
            "google-genai 1.55.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 11.0.3 which is incompatible.\n",
            "tobler 0.13.0 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tobler 0.13.0 requires tqdm>=4.67, but you have tqdm 4.66.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0müöÄ –ó–∞–ø—É—Å–∫–∞—é –Ω–µ–π—Ä–æ—Å–µ—Ç—å... (–ü–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –∑–∞–π–º–µ—Ç 5-7 –º–∏–Ω—É—Ç –Ω–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π)\n",
            "‚è≥ –ö–æ–≥–¥–∞ –ø–æ—è–≤–∏—Ç—Å—è —Å—Å—ã–ª–∫–∞ –≤–∏–¥–∞ 'Running on public URL: https://...gradio.live', –Ω–∞–∂–º–∏—Ç–µ –Ω–∞ –Ω–µ—ë.\n",
            "Already up-to-date\n",
            "Update succeeded.\n",
            "[System ARGV] ['entry_with_update.py', '--preset', 'realistic', '--always-high-vram', '--share']\n",
            "/content/Fooocus/build_launcher.py:9: SyntaxWarning: invalid escape sequence '\\p'\n",
            "  .\\python_embeded\\python.exe -s Fooocus\\entry_with_update.py {cmds} %*\n",
            "Python 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
            "Fooocus version: 2.5.5\n",
            "Loaded preset: /content/Fooocus/presets/realistic.json\n",
            "[Cleanup] Attempting to delete content of temp dir /tmp/fooocus\n",
            "[Cleanup] Cleanup successful\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/xlvaeapp.pth\" to /content/Fooocus/models/vae_approx/xlvaeapp.pth\n",
            "\n",
            "100% 209k/209k [00:00<00:00, 676kB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/vaeapp_sd15.pt\" to /content/Fooocus/models/vae_approx/vaeapp_sd15.pth\n",
            "\n",
            "100% 209k/209k [00:00<00:00, 677kB/s]\n",
            "Downloading: \"https://huggingface.co/mashb1t/misc/resolve/main/xl-to-v1_interposer-v4.0.safetensors\" to /content/Fooocus/models/vae_approx/xl-to-v1_interposer-v4.0.safetensors\n",
            "\n",
            "100% 5.40M/5.40M [00:00<00:00, 17.3MB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_expansion.bin\" to /content/Fooocus/models/prompt_expansion/fooocus_expansion/pytorch_model.bin\n",
            "\n",
            "100% 335M/335M [00:06<00:00, 56.3MB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/fav_models/resolve/main/fav/realisticStockPhoto_v20.safetensors\" to /content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors\n",
            "\n",
            "100% 6.46G/6.46G [01:18<00:00, 88.4MB/s]\n",
            "Downloading: \"https://huggingface.co/mashb1t/fav_models/resolve/main/fav/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors\" to /content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors\n",
            "\n",
            "100% 870M/870M [00:10<00:00, 83.1MB/s]\n",
            "Total VRAM 15095 MB, total RAM 12976 MB\n",
            "Set vram state to: HIGH_VRAM\n",
            "Always offload VRAM\n",
            "Device: cuda:0 Tesla T4 : native\n",
            "VAE dtype: torch.float32\n",
            "Using pytorch cross attention\n",
            "/content/Fooocus/ldm_patched/unipc/uni_pc.py:56: SyntaxWarning: invalid escape sequence '\\h'\n",
            "  The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n",
            "Refiner unloaded.\n",
            "Running on local URL:  http://127.0.0.1:7865\n",
            "IMPORTANT: You are using gradio version 3.41.2, however version 4.44.1 is available, please upgrade.\n",
            "--------\n",
            "Running on public URL: https://71f3439fdec30cff17.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "model_type EPS\n",
            "UNet ADM Dimension 2816\n",
            "Using pytorch attention in VAE\n",
            "Working with z of shape (1, 4, 32, 32) = 4096 dimensions.\n",
            "Using pytorch attention in VAE\n",
            "extra {'cond_stage_model.clip_l.logit_scale', 'cond_stage_model.clip_l.text_projection'}\n",
            "left over keys: dict_keys(['cond_stage_model.clip_l.transformer.text_model.embeddings.position_ids'])\n",
            "loaded straight to GPU\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "Base model loaded: /content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors\n",
            "VAE loaded: None\n",
            "Request to load LoRAs [('SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors', 0.25)] for model [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for UNet [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 722 keys at weight 0.25.\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for CLIP [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 264 keys at weight 0.25.\n",
            "Fooocus V2 Expansion: Vocab with 642 words.\n",
            "Fooocus Expansion engine loaded for cuda:0, use_fp16 = True.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.32 seconds\n",
            "2026-01-29 03:46:57.843852: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1769658418.079732    1647 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1769658418.141819    1647 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1769658418.607346    1647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769658418.607384    1647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769658418.607391    1647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1769658418.607397    1647 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-29 03:46:58.653286: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Started worker with PID 964\n",
            "App started successful. Use the app with http://127.0.0.1:7865/ or 127.0.0.1:7865 or https://71f3439fdec30cff17.gradio.live\n",
            "(1006, '')\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 1066661927123537270\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading control models ...\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/clip_vision_vit_h.safetensors\" to /content/Fooocus/models/clip_vision/clip_vision_vit_h.safetensors\n",
            "\n",
            "100% 1.84G/1.84G [00:21<00:00, 93.2MB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_ip_negative.safetensors\" to /content/Fooocus/models/controlnet/fooocus_ip_negative.safetensors\n",
            "\n",
            "100% 64.1k/64.1k [00:00<00:00, 210kB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/ip-adapter-plus_sdxl_vit-h.bin\" to /content/Fooocus/models/controlnet/ip-adapter-plus_sdxl_vit-h.bin\n",
            "\n",
            "100% 967M/967M [00:17<00:00, 59.6MB/s]\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] A young beautiful brunette woman with short hair comes out of the azure ocean without a swimsuit, full detailed, cinematic, light shining, intricate, elegant, highly decorated, very coherent, strong, vibrant color, great composition, sharp focus, cool, inspiring, thought, iconic, stunning, incredible, epic, dynamic, ambient, shiny, illuminated, extremely complex\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] A young beautiful brunette woman with short hair comes out of the azure ocean without a swimsuit, detailed, dramatic, intricate, elegant, highly graceful, light, sharp focus, detail, very gentle flowing, professional, inspired, color spread, extremely nice, generous, amazing, inspiring, attractive, smart, artistic, positive, cheerful, cute, pretty, innocent\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.27 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Requested to load CLIPVisionModelWithProjection\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.77 seconds\n",
            "Requested to load Resampler\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.45 seconds\n",
            "Requested to load To_KV\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.28 seconds\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1152, 896)\n",
            "Preparation time: 52.74 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 3.79 seconds\n",
            "100% 30/30 [00:24<00:00,  1.22it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.26 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2026-01-29/log.html\n",
            "Generating and saving time: 31.09 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.75 seconds\n",
            "100% 30/30 [00:24<00:00,  1.23it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.25 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2026-01-29/log.html\n",
            "Generating and saving time: 28.05 seconds\n",
            "[Enhance] Skipping, preconditions aren't met\n",
            "Processing time (total): 59.14 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 111.94 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 2.02 seconds\n",
            "(1006, '')\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 1066661927123537270\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading control models ...\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1209, in handler\n",
            "    apply_control_nets(async_task, height, ip_adapter_face_path, ip_adapter_path, width, current_progress)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 422, in apply_control_nets\n",
            "    cn_img = HWC3(cn_img)\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/util.py\", line 134, in HWC3\n",
            "    assert x.dtype == np.uint8\n",
            "           ^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'dtype'\n",
            "Total time: 0.63 seconds\n",
            "(1006, '')\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 1066661927123537270\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading control models ...\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 1 new model\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.13 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1209, in handler\n",
            "    apply_control_nets(async_task, height, ip_adapter_face_path, ip_adapter_path, width, current_progress)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 422, in apply_control_nets\n",
            "    cn_img = HWC3(cn_img)\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/util.py\", line 134, in HWC3\n",
            "    assert x.dtype == np.uint8\n",
            "           ^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'dtype'\n",
            "Total time: 0.74 seconds\n",
            "(1006, '')\n",
            "(1006, '')\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 5473295729211478107\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading control models ...\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/ip-adapter-plus-face_sdxl_vit-h.bin\" to /content/Fooocus/models/controlnet/ip-adapter-plus-face_sdxl_vit-h.bin\n",
            "\n",
            "100% 967M/967M [00:21<00:00, 47.6MB/s]\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 1 new model\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] A young beautiful brunette woman with short hair comes out of the azure ocean in a wet white transparent swimsuit, detailed gorgeous very cute aesthetic, dramatic bright colors, charming, surreal, innocent, pretty, perfect, cinematic, intricate, highly artistic, vibrant, sharp focus, glamorous, complex, extremely attractive, designed, rich background, professional, fancy, expressive, elegant\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] A young beautiful brunette woman with short hair comes out of the azure ocean in a wet white transparent swimsuit, detailed deep light, delicate, clear focus, highly intricate, innocent, confident, elegant, aesthetic, sublime, magical, dramatic ambient background, sharp detail, rich vibrant colors, epic composition, vivid, romantic, iconic, fine, best, cinematic, modern\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Requested to load CLIPVisionModelWithProjection\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.75 seconds\n",
            "Requested to load Resampler\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.50 seconds\n",
            "Requested to load To_KV\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.26 seconds\n",
            "Requested to load CLIPVisionModelWithProjection\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.55 seconds\n",
            "Requested to load Resampler\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.42 seconds\n",
            "Requested to load To_KV\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.22 seconds\n",
            "Downloading: \"https://github.com/xinntao/facexlib/releases/download/v0.1.0/detection_Resnet50_Final.pth\" to /content/Fooocus/models/controlnet/detection_Resnet50_Final.pth\n",
            "\n",
            "100% 104M/104M [00:00<00:00, 215MB/s] \n",
            "Downloading: \"https://github.com/xinntao/facexlib/releases/download/v0.2.2/parsing_parsenet.pth\" to /content/Fooocus/models/controlnet/parsing_parsenet.pth\n",
            "\n",
            "100% 81.4M/81.4M [00:04<00:00, 20.6MB/s]\n",
            "Detected 1 faces\n",
            "Requested to load CLIPVisionModelWithProjection\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.59 seconds\n",
            "Requested to load Resampler\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.45 seconds\n",
            "Requested to load To_KV\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.28 seconds\n",
            "Detected 1 faces\n",
            "Requested to load CLIPVisionModelWithProjection\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.85 seconds\n",
            "Requested to load Resampler\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.46 seconds\n",
            "Requested to load To_KV\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.25 seconds\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1152, 896)\n",
            "Preparation time: 46.75 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 3.68 seconds\n",
            "100% 30/30 [00:24<00:00,  1.22it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.24 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2026-01-29/log.html\n",
            "Generating and saving time: 30.99 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.73 seconds\n",
            "100% 30/30 [00:24<00:00,  1.20it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.24 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2026-01-29/log.html\n",
            "Generating and saving time: 28.55 seconds\n",
            "[Enhance] Skipping, preconditions aren't met\n",
            "Processing time (total): 59.54 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.92 seconds\n",
            "(1006, '')\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 5473295729211478107\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading control models ...\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "Total time: 0.61 seconds\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1209, in handler\n",
            "    apply_control_nets(async_task, height, ip_adapter_face_path, ip_adapter_path, width, current_progress)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 422, in apply_control_nets\n",
            "    cn_img = HWC3(cn_img)\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/util.py\", line 134, in HWC3\n",
            "    assert x.dtype == np.uint8\n",
            "           ^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'dtype'\n",
            "(1006, '')\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 5473295729211478107\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading control models ...\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 1 new model\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "Total time: 0.13 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1209, in handler\n",
            "    apply_control_nets(async_task, height, ip_adapter_face_path, ip_adapter_path, width, current_progress)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 422, in apply_control_nets\n",
            "    cn_img = HWC3(cn_img)\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/util.py\", line 134, in HWC3\n",
            "    assert x.dtype == np.uint8\n",
            "           ^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'dtype'\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 2014143515421634273\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading control models ...\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 1 new model\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] A young beautiful brunette woman with short hair comes out of the azure ocean in a wet white open transparent swimsuit, detailed full theatrical cinematic flowing celestial light, coherent composition, dramatic atmosphere, sharp focus, perfect professional, highly colorful, aesthetic, joyful, mystical, magic, vibrant, best detail, pretty, glowing, striking, attractive, illustrious, singular, very artistic\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] A young beautiful brunette woman with short hair comes out of the azure ocean in a wet white open transparent swimsuit, detailed full pretty, attractive, breathtaking, dramatic ambient light, dynamic background, intricate, elegant, highly coherent, sharp focus, inspired, vibrant colors, complete, very inspirational, original, gorgeous, thought formed, saturated, perfect composition, color, epic\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Detected 1 faces\n",
            "Requested to load CLIPVisionModelWithProjection\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.75 seconds\n",
            "Requested to load Resampler\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.49 seconds\n",
            "Requested to load To_KV\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.27 seconds\n",
            "Detected 1 faces\n",
            "Requested to load CLIPVisionModelWithProjection\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.56 seconds\n",
            "Requested to load Resampler\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.47 seconds\n",
            "Requested to load To_KV\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.23 seconds\n",
            "[Parameters] Denoising Strength = 1.0\n",
            "[Parameters] Initial Latent shape: Image Space (1152, 896)\n",
            "Preparation time: 12.10 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.77 seconds\n",
            "100% 30/30 [00:24<00:00,  1.23it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.26 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2026-01-29/log.html\n",
            "Generating and saving time: 27.87 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.78 seconds\n",
            "100% 30/30 [00:24<00:00,  1.21it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.27 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2026-01-29/log.html\n",
            "Generating and saving time: 28.34 seconds\n",
            "[Enhance] Skipping, preconditions aren't met\n",
            "Processing time (total): 56.21 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 68.36 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.96 seconds\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/model_base_caption_capfilt_large.pth\" to /content/Fooocus/models/clip_vision/model_base_caption_capfilt_large.pth\n",
            "\n",
            "100% 855M/855M [00:29<00:00, 30.8MB/s]\n",
            "load checkpoint from /content/Fooocus/models/clip_vision/model_base_caption_capfilt_large.pth\n",
            "Requested to load BLIP_Decoder\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.79 seconds\n",
            "(1006, '')\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 2014143515421634273\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading control models ...\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.96 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.21 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1209, in handler\n",
            "    apply_control_nets(async_task, height, ip_adapter_face_path, ip_adapter_path, width, current_progress)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 432, in apply_control_nets\n",
            "    cn_img = HWC3(cn_img)\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/util.py\", line 134, in HWC3\n",
            "    assert x.dtype == np.uint8\n",
            "           ^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'dtype'\n",
            "Total time: 2.71 seconds\n",
            "(1006, '')\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 2014143515421634273\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading control models ...\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 1 new model\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1209, in handler\n",
            "    apply_control_nets(async_task, height, ip_adapter_face_path, ip_adapter_path, width, current_progress)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 432, in apply_control_nets\n",
            "    cn_img = HWC3(cn_img)\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/util.py\", line 134, in HWC3\n",
            "    assert x.dtype == np.uint8\n",
            "           ^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'dtype'\n",
            "Total time: 0.73 seconds\n",
            "(1006, '')\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 2014143515421634273\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading control models ...\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 15\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Refiner unloaded.\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 1 new model\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.12 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1471, in worker\n",
            "    handler(task)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 120, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 1209, in handler\n",
            "    apply_control_nets(async_task, height, ip_adapter_face_path, ip_adapter_path, width, current_progress)\n",
            "  File \"/content/Fooocus/modules/async_worker.py\", line 432, in apply_control_nets\n",
            "    cn_img = HWC3(cn_img)\n",
            "             ^^^^^^^^^^^^\n",
            "  File \"/content/Fooocus/modules/util.py\", line 134, in HWC3\n",
            "    assert x.dtype == np.uint8\n",
            "           ^^^^^^^\n",
            "AttributeError: 'tuple' object has no attribute 'dtype'\n",
            "Total time: 0.74 seconds\n",
            "Downloading data from 'https://github.com/danielgatis/rembg/releases/download/v0.0.0/isnet-general-use.onnx' to file '/content/Fooocus/models/inpaint/isnet-general-use.onnx'.\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 179M/179M [00:00<00:00, 764GB/s]\n",
            "[Parameters] Adaptive CFG = 7\n",
            "[Parameters] CLIP Skip = 2\n",
            "[Parameters] Sharpness = 2\n",
            "[Parameters] ControlNet Softness = 0.25\n",
            "[Parameters] ADM Scale = 1.5 : 0.8 : 0.3\n",
            "[Parameters] Seed = 3976955151990673950\n",
            "[Parameters] CFG = 3\n",
            "[Fooocus] Downloading upscale models ...\n",
            "Downloading: \"https://huggingface.co/lllyasviel/misc/resolve/main/fooocus_upscaler_s409985e5.bin\" to /content/Fooocus/models/upscale_models/fooocus_upscaler_s409985e5.bin\n",
            "\n",
            "100% 32.1M/32.1M [00:00<00:00, 55.1MB/s]\n",
            "[Fooocus] Downloading inpainter ...\n",
            "Downloading: \"https://huggingface.co/lllyasviel/fooocus_inpaint/resolve/main/fooocus_inpaint_head.pth\" to /content/Fooocus/models/inpaint/fooocus_inpaint_head.pth\n",
            "\n",
            "100% 51.4k/51.4k [00:00<00:00, 137kB/s]\n",
            "Downloading: \"https://huggingface.co/lllyasviel/fooocus_inpaint/resolve/main/inpaint_v26.fooocus.patch\" to /content/Fooocus/models/inpaint/inpaint_v26.fooocus.patch\n",
            "\n",
            "100% 1.23G/1.23G [01:02<00:00, 21.1MB/s]\n",
            "[Inpaint] Current inpaint model is /content/Fooocus/models/inpaint/inpaint_v26.fooocus.patch\n",
            "[Fooocus] Loading control models ...\n",
            "[Parameters] Sampler = dpmpp_2m_sde_gpu - karras\n",
            "[Parameters] Steps = 30 - 24\n",
            "[Fooocus] Initializing ...\n",
            "[Fooocus] Loading models ...\n",
            "Synthetic Refiner Activated\n",
            "Synthetic Refiner Activated\n",
            "Request to load LoRAs [('SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors', 0.25), ('/content/Fooocus/models/inpaint/inpaint_v26.fooocus.patch', 1.0)] for model [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for UNet [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 722 keys at weight 0.25.\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for CLIP [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 264 keys at weight 0.25.\n",
            "Loaded LoRA [/content/Fooocus/models/inpaint/inpaint_v26.fooocus.patch] for UNet [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 960 keys at weight 1.0.\n",
            "Request to load LoRAs [('SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors', 0.25)] for model [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors].\n",
            "Loaded LoRA [/content/Fooocus/models/loras/SDXL_FILM_PHOTOGRAPHY_STYLE_V1.safetensors] for UNet [/content/Fooocus/models/checkpoints/realisticStockPhoto_v20.safetensors] with 722 keys at weight 0.25.\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "unload clone 0\n",
            "[Fooocus Model Management] Moving model(s) has taken 2.26 seconds\n",
            "[Fooocus] Processing prompts ...\n",
            "[Fooocus] Preparing Fooocus text #1 ...\n",
            "[Prompt Expansion] a pretty young lady in a little pink underwear, intricate, highly detailed, wonderful delicate stunning fine detail directed sharp focus shining light, mystical colors, very coherent, cinematic, amazing composition, elegant, perfect full color, aesthetic, spectacular, awesome atmosphere, creative, beautiful, marvelous, luxury, brilliant, epic, gorgeous, dramatic, illuminated background, professional, best, outstanding\n",
            "[Fooocus] Preparing Fooocus text #2 ...\n",
            "[Prompt Expansion] a pretty young lady in a little pink underwear, detailed intricate bright colorful, magic, divine, magical, flowing, light, sharp focus, dramatic cinematic, fine detail, highly elaborate, elegant, open background, relaxed, professional, artistic, extremely inspirational, wonderful, cute, lovely, coherent, vibrant, color, complex, brilliant, symmetry, clear, aesthetic, stunning\n",
            "[Fooocus] Encoding positive #1 ...\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.13 seconds\n",
            "[Fooocus] Encoding positive #2 ...\n",
            "[Fooocus] Encoding negative #1 ...\n",
            "[Fooocus] Encoding negative #2 ...\n",
            "[Fooocus] Image processing ...\n",
            "[Fooocus] VAE Inpaint encoding ...\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.49 seconds\n",
            "[Fooocus] VAE encoding ...\n",
            "Final resolution is (768, 1344), latent is (768, 1344).\n",
            "[Parameters] Denoising Strength = 1\n",
            "[Parameters] Initial Latent shape: torch.Size([1, 4, 168, 96])\n",
            "Preparation time: 88.65 seconds\n",
            "Using karras scheduler.\n",
            "[Fooocus] Preparing task 1/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 4.24 seconds\n",
            " 80% 24/30 [00:20<00:04,  1.20it/s]Requested to load SDXL\n",
            "Loading 1 new model\n",
            "unload clone 0\n",
            "[Fooocus Model Management] Moving model(s) has taken 3.03 seconds\n",
            "Refiner Swapped\n",
            "100% 30/30 [00:28<00:00,  1.06it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.28 seconds\n",
            "[Fooocus] Saving image 1/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2026-01-29/log.html\n",
            "Generating and saving time: 35.55 seconds\n",
            "[Fooocus] Preparing task 2/2 ...\n",
            "[Sampler] refiner_swap_method = joint\n",
            "[Sampler] sigma_min = 0.0291671771556139, sigma_max = 14.614643096923828\n",
            "Requested to load SDXL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 3.32 seconds\n",
            " 80% 24/30 [00:20<00:05,  1.18it/s]Requested to load SDXL\n",
            "Loading 1 new model\n",
            "unload clone 0\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.86 seconds\n",
            "Refiner Swapped\n",
            "100% 30/30 [00:27<00:00,  1.11it/s]\n",
            "Requested to load AutoencoderKL\n",
            "Loading 1 new model\n",
            "[Fooocus Model Management] Moving model(s) has taken 0.27 seconds\n",
            "[Fooocus] Saving image 2/2 to system ...\n",
            "Image generated with private log at: /content/Fooocus/outputs/2026-01-29/log.html\n",
            "Generating and saving time: 33.33 seconds\n",
            "[Enhance] Skipping, preconditions aren't met\n",
            "Processing time (total): 68.89 seconds\n",
            "Requested to load SDXLClipModel\n",
            "Requested to load GPT2LMHeadModel\n",
            "Loading 2 new models\n",
            "Total time: 157.60 seconds\n",
            "[Fooocus Model Management] Moving model(s) has taken 1.85 seconds\n"
          ]
        }
      ]
    }
  ]
}